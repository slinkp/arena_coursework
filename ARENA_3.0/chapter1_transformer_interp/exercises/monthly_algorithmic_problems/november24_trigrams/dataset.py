import torch as t
from torch import Tensor
from torch.utils.data import Dataset
from tqdm import tqdm


class BigramDataset(Dataset):
    def __init__(
        self,
        size: int,
        seq_len: int,
        d_vocab: int,
        n_trigrams: int | None = None,
        trigram_prob: float | None = None,
        device: str = "cpu",
        seed: int = 42,
    ):
        """
        Dataset that generates sequences of tokens where certain trigrams determine the third token based on predefined
        trigrams.

        You can specify the total number of trigrams of the form (a, b -> c) based on either `n_trigrams` (in which case
        we'll choose this many randomly) or `trigram_prob` (in which case we'll choose a number so that for any randomly
        chosen 2 adjacent tokens in our sequence, this is the probability of the third token being determined by a trigram.
        """
        assert (trigram_prob is not None) ^ (
            n_trigrams is not None
        ), "Must specify either trigram_prob or n_trigrams, not both"

        if trigram_prob is None:
            self.trigram_prob = n_trigrams / (d_vocab * (d_vocab - 1))
            self.n_trigrams = n_trigrams
        else:
            self.trigram_prob = trigram_prob
            self.n_trigrams = int(self.trigram_prob * d_vocab * (d_vocab - 1))

        if self.trigram_prob > 0.2:
            print(f"Bigram prob is high ({self.trigram_prob:.2%}), consider a larger vocab size or fewer trigrams.")

        self.seq_len = seq_len
        self.d_vocab = d_vocab
        self.device = device
        self.seed = seed
        t.manual_seed(seed)

        self.trigrams = self.generate_trigrams(self.n_trigrams)
        self.vocab = list(range(d_vocab))
        self.toks = self.generate_batch(size, device)

    def generate_trigrams(self, n_trigrams):
        """
        Generates a set of trigrams. Ensures no chains (e.g. 123, 231) or contradictions (e.g. 123, 124). Does this by
        first checking the parameters are large enough to avoid these issues with high probability, and then randomly
        sampling trigrams until the conditions are met.
        """
        trigrams = set()
        while len(trigrams) < n_trigrams:
            a, b, c = t.randint(0, self.d_vocab, (3,)).tolist()
            if not self.is_contradictory(trigrams, (a, b, c)) and not self.has_chain(trigrams, (a, b, c)):
                trigrams.add((a, b, c))
        return trigrams

    def generate_batch(self, size: int, device: str):
        toks = t.randint(0, self.d_vocab, (size, self.seq_len), device=device)
        for i in tqdm(range(self.seq_len - 2), desc="Generating dataset"):
            for a, b, c in self.trigrams:
                toks[(toks[:, i] == a) & (toks[:, i + 1] == b), i + 2] = c
        return toks

    @property
    def all_trigrams_batch(self) -> Tensor:
        """
        Returns a dataset of one of each of the model's trigrams.
        """
        return t.tensor(list(self.trigrams), device=self.device)

    @staticmethod
    def is_contradictory(trigrams, new_trigram):
        for x, y, z in trigrams:
            if (x, y) == new_trigram[:2] and z != new_trigram[2]:
                return True
        return False

    @staticmethod
    def has_chain(trigrams, new_trigram):
        for x, y, z in trigrams:
            if (x, y) == new_trigram[1:] or (y, z) == new_trigram[:2]:
                return True
        return False

    def __len__(self):
        return len(self.toks)

    def __getitem__(self, idx):
        return self.toks[idx]
