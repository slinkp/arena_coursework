import gc
import time
from copy import deepcopy
from dataclasses import dataclass

import einops
import torch as t
import torch.nn.functional as F
import wandb
from eindex import eindex
from jaxtyping import Float, Int
from monthly_algorithmic_problems.november24_trigrams.dataset import BigramDataset
from monthly_algorithmic_problems.november24_trigrams.model import create_model
from regex import D
from torch import Tensor
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
from transformer_lens.HookedTransformer import HookedTransformer


@dataclass
class TrainArgs:
    #
    # Dataset
    d_vocab: int
    seq_len: int
    trigram_prob: float | None
    n_trigrams: int | None
    #
    # Training
    trainset_size: int
    valset_size: int
    epochs: int
    batch_size: int
    lr_start: float
    lr_end: float
    weight_decay: float
    #
    # Model architecture
    d_model: int
    d_head: int
    n_layers: int
    n_heads: int
    d_mlp: int
    normalization_type: str | None
    #
    # Misc.
    seed: int
    device: str
    use_wandb: bool = True
    wandb_project: str = "alg-challenge-trigrams-nov24"
    wandb_name: str | None = None


class Trainer:
    def __init__(self, args: TrainArgs):
        self.args = args
        self.model = create_model(**args.__dict__)
        self.dataset = BigramDataset(
            size=args.trainset_size + args.valset_size,
            seq_len=args.seq_len,
            d_vocab=args.d_vocab,
            n_trigrams=args.n_trigrams,
            trigram_prob=args.trigram_prob,
            seed=args.seed,
        )
        self.train_dataset, self.val_dataset = random_split(self.dataset, [args.trainset_size, args.valset_size])
        if self.args.use_wandb:
            wandb.init(project=args.wandb_project, name=args.wandb_name, config=args.__dict__)
            # wandb.watch(self.model)

    def training_step(self, toks: Tensor) -> Tensor:
        logits, target = self._shared_train_validation_step(toks)
        # logprobs = logits.log_softmax(-1)
        loss = F.cross_entropy(
            einops.rearrange(logits, "batch seq vocab_out -> (batch seq) vocab_out"),
            einops.rearrange(target, "batch seq -> (batch seq)"),
        )
        return loss

    def validation_step(self, toks: Tensor) -> tuple[int, float]:
        logits, target = self._shared_train_validation_step(toks)
        accuracy = (logits.argmax(-1) == target).int().sum().item()
        avg_prob = eindex(logits.softmax(-1), target, "batch seq [batch seq]").float().mean().item()
        return accuracy, avg_prob

    def _shared_train_validation_step(
        self, toks: Tensor
    ) -> tuple[Float[Tensor, "batch seq d_vocab"], Int[Tensor, "batch seq"]]:
        toks = toks.to(self.args.device)
        logits = self.model(toks)[:, :-1]
        return logits, toks[:, 1:]

    def train(self) -> HookedTransformer:
        optimizer = t.optim.AdamW(self.model.parameters(), lr=self.args.lr_start, weight_decay=self.args.weight_decay)
        # we want to scale loss so 1 = uniform, 0 = optimal performance on this dataset (optimal performance is getting
        # zero loss when you're on the second token of a trigram)
        unif_loss = t.tensor(self.args.d_vocab).log().item()
        min_loss = unif_loss * (1 - self.args.trigram_prob)

        train_dataloader = DataLoader(self.train_dataset, batch_size=self.args.batch_size, shuffle=False)
        val_dataloader = DataLoader(self.val_dataset, batch_size=self.args.batch_size, shuffle=False)

        # best_model = deepcopy(self.model)
        best_epoch = None
        best_accuracy = None
        n_tokens = 0

        formats = {
            "details/epoch": "02",
            "details/lr": ".2e",
            "metrics/train_loss_as_frac": ".2%",
            "metrics/trigram_n_correct": "03",
        }

        for epoch in range(self.args.epochs):
            lr = self.args.lr_start + (self.args.lr_end - self.args.lr_start) * ((epoch + 1) / self.args.epochs)
            optimizer.param_groups[0]["lr"] = lr
            t0 = time.monotonic()

            for batch in train_dataloader:
                n_tokens += batch.numel()
                loss = self.training_step(batch)
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                loss_as_frac = (loss.item() - min_loss) / (unif_loss - min_loss)
                metrics = {
                    "details/epoch": epoch,
                    "metrics/train_loss": loss.item(),
                    "metrics/train_loss_as_frac": loss_as_frac,
                    "details/lr": lr,
                }
                print(
                    ", ".join(f"{k.split('/')[1]}: {v:{formats.get(k, '.4f')}}" for k, v in metrics.items()), end="\r"
                )
                if self.args.use_wandb:
                    wandb.log(metrics, step=n_tokens)

            test_metrics = self.get_test_metrics(val_dataloader) | {"details/time_for_epoch": time.monotonic() - t0}
            metrics |= test_metrics
            print(", ".join(f"{k.split('/')[1]}: {v:{formats.get(k, '.4f')}}" for k, v in metrics.items()))
            if self.args.use_wandb:
                wandb.log(test_metrics, step=n_tokens)

            accuracy = metrics.get("metrics/trigram_avg_correct_prob", 0.0)
            if best_accuracy is None or accuracy > best_accuracy:
                best_epoch = epoch
                best_accuracy = accuracy
                # best_model = deepcopy(self.model)

            t.cuda.empty_cache()
            gc.collect()

        if self.args.use_wandb:
            wandb.finish()

        print(f"Returning best model from epoch {best_epoch}/{self.args.epochs}, with accuracy {best_accuracy:.3f}")
        # return best_model
        return self.model

    @t.inference_mode()
    def get_test_metrics(self, val_dataloader: DataLoader):
        """
        TODO - fill this out with more stuff, but for now it's just accuracy and avg prob
        """
        # # Get stats on holdout set
        # accuracy_list = [self.validation_step(batch) for batch in val_dataloader]
        # accuracy = sum([x[0] for x in accuracy_list]) / (self.args.valset_size * (self.args.seq_len - 1))
        # avg_prob = sum([x[1] for x in accuracy_list]) / len(accuracy_list)

        # Get stats on dataset consisting only of trigrams
        # logits, cache = self.model.run_with_cache(self.dataset.all_trigrams_batch)
        logits = self.model(self.dataset.all_trigrams_batch)
        probs = logits[:, 1].softmax(-1)
        targets = self.dataset.all_trigrams_batch[:, 2].to(probs.device)
        accuracy_trigrams = (probs.argmax(-1) == targets).int().sum().item()
        correct_probs = eindex(probs, targets, "batch [batch]").mean().item()
        return {
            # "Accuracy (test set)": accuracy,
            # "Avg correct prob (test set)": avg_prob,
            "metrics/trigram_n_correct": accuracy_trigrams,
            "metrics/trigram_frac_correct": accuracy_trigrams / self.dataset.n_trigrams,
            "metrics/trigram_avg_correct_prob": correct_probs,
        }
